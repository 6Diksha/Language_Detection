{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJBpumC+8TDBey4raeQH+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/6Diksha/Language_Detection/blob/main/Language_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2LqosP-v3Qq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xbL2pzgVyTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#asking the path of the csv file that contains the dataset of tweets as input from user(the csv file is stored on the drive)\n",
        "path_tweet = input(\"Enter path to the Tweets Dataset[csv file]: \")  \n",
        "data_tweet = pd.read_csv(path_tweet) #reading the csv file \n",
        "\n",
        "#asking the heading of the column in the csv file under which all the tweets were listed as input from the user\n",
        "column_heading = input(\"Enter the heading of the column that contains the tweets(case-sensitive): \")\n",
        "\n",
        "saved_column = data_tweet[column_heading] #saving only the specific column which contains the tweets as another variable"
      ],
      "metadata": {
        "id": "swZQr-Lwyc2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the language detection dataset(for 17 languages) from github(raw data)\n",
        "#a csv file from github(raw data) can be imported by pasting the url of that page(in Google Colab)\n",
        "url = \"https://raw.githubusercontent.com/basil-b2s/Language-Detector/master/language_detection.csv\"\n",
        "data_lang = pd.read_csv(url) #reading the csv file"
      ],
      "metadata": {
        "id": "E6tDwYFVykYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning the sample text and its corresponding language from the language dataset to different variables\n",
        "dataset_text = data_lang[\"Text\"] #independent variable\n",
        "dataset_language = data_lang[\"Language\"] #dependent varaiable"
      ],
      "metadata": {
        "id": "EjhR60oHyocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "dataset_language = le.fit_transform(dataset_language)"
      ],
      "metadata": {
        "id": "lmwW1sFMysLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the data/formatting the data\n",
        "data_formatted = []\n",
        " \n",
        "for text in dataset_text:\n",
        "  text = re.sub(r'[!@#$(),\\n\"%^*?\\:;~`0-9]', ' ', text) #removing numbers and special characters\n",
        "  text = re.sub(r'[[]]', ' ', text)\n",
        "  text = text.lower() #converting the data to lowercase\n",
        "  data_formatted.append(text)"
      ],
      "metadata": {
        "id": "sIFrnHxfys0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting text using features into a vector\n",
        "#BoW Model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer() #assigning short name\n",
        "dataset_text = cv.fit_transform(data_formatted).toarray() #an array of frequency of features\n",
        "dataset_text.shape  "
      ],
      "metadata": {
        "id": "qaVSdIedys6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_text_train, dataset_text_test, dataset_language_train, dataset_language_test = train_test_split(dataset_text, dataset_language, test_size = 0.20) #the ratio/size is 0.20"
      ],
      "metadata": {
        "id": "b243N2Izyzea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using Naive-Bayes model\n",
        "#model creation\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB() \n",
        "model.fit(dataset_text_train, dataset_language_train) #fitting into the NB model"
      ],
      "metadata": {
        "id": "N6VcqAbDy4oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_language_pred = model.predict(dataset_text_test) "
      ],
      "metadata": {
        "id": "L8Ho0jyiy5Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "ac = accuracy_score(dataset_language_test, dataset_language_pred) #prediction of accuracy\n",
        "print(\"Accuracy: \", ac)"
      ],
      "metadata": {
        "id": "aUCpM2Lcy89x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "plt.figure(figsize = (15, 15))\n",
        "cm = confusion_matrix(dataset_language_test, dataset_language_pred)\n",
        "sns.heatmap(cm, annot = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Io9qCCgqy9ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text): #text variable is the text in the tweet\n",
        "  dataset_text = cv.transform([text]).toarray() #BoW\n",
        "  lang = model.predict(dataset_text) #Naive-Bayes\n",
        "  lang = le.inverse_transform(lang) #transformed back to original language\n",
        "  print(\"The language is:\", lang[0])"
      ],
      "metadata": {
        "id": "3tCwR9nFzBHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower = int(input(\"Enter lower limit: \")) #entering the range\n",
        "upper = int(input(\"Enter upper limit: \"))\n",
        "for i in saved_column[0:100]: #iterating over a specific range, in order to avoid any runtime errors\n",
        "  print(i)\n",
        "  predict(i)"
      ],
      "metadata": {
        "id": "xlJhy68BzDGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}